{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [1, 1, 1, 16]             144\n",
      "            Linear-2               [1, 1, 1, 8]             136\n",
      "            Linear-3               [1, 1, 1, 1]               9\n",
      "================================================================\n",
      "Total params: 289\n",
      "Trainable params: 289\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 自定义神经网络,CNN,当然现在没有卷积层，也可以叫NN\n",
    "# 输入数据的尺寸\n",
    "hdreshape = 8\n",
    "# 隐藏层1神经元个数\n",
    "hdlayer_1 = 16\n",
    "# 隐藏层2神经元个数\n",
    "hdlayer_2 = 8\n",
    "# 隐藏层3神经元个数\n",
    "hdlayer_3 = 256\n",
    "# 建立pytorch的神经网络类，可以看到基于nn.moudle生成的，包含初始化_init_,前向传播（也就是网络结构）\n",
    "# forward 中有很多注释掉的层，实际上我们正是在这里修改网络结构，目前我只用到了fc1,fc2和out三个。所有的网络层都需要先在初始化定义好\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32,\\\n",
    "             kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32,\\\n",
    "             kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, \\\n",
    "            kernel_size=3, padding=0)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=hdreshape, out_features=hdlayer_1)\n",
    "        self.fc2 = nn.Linear(in_features=hdlayer_1, out_features=hdlayer_2)\n",
    "        # self.fc3 = nn.Linear(in_features=hdlayer_2, out_features=hdlayer_3)\n",
    "        self.out = nn.Linear(in_features=hdlayer_2, out_features=1)\n",
    "        self.dr1 = nn.Dropout2d(0.2)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "        # t = t.reshape(5,12)\n",
    "        # t = t.unsqueeze(0)\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        # t = self.conv1(t)\n",
    "        # t = F.relu(t)\n",
    "        # t = F.max_pool2d(t, kernel_size=2, stride=1)\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        # t = self.conv2(t)\n",
    "        # t = F.relu(t)\n",
    "        # t = self.dr1(t)\n",
    "        # t = F.max_pool2d(t, kernel_size=2, stride=1)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        # t = t.reshape(-1, hdreshape)\n",
    "        # t = t.flatten(start_dim=0)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "        # t = self.fc3(t)\n",
    "        # t = F.relu(t)\n",
    "        # t = self.dr1(t)\n",
    "\n",
    "        # (5) output layer\n",
    "        t = self.out(t)\n",
    "\n",
    "        return t\n",
    "# 基于Network 类生成net对象\n",
    "net = Network()\n",
    "\n",
    "# 打印网络，检查输入输出 shape是否正确\n",
    "# print(net)\n",
    "summary(net,(1,1,8),batch_size = 1,device = \"cpu\")\n",
    "# 可视化结构，torchviz\n",
    "sampleInput = torch.randn(1,1,1,8).requires_grad_(True)\n",
    "sampleOutput = net(sampleInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取xlsx文件，这部分直接使用了之前的代码\n",
    "data = pd.read_excel('dataset.xlsx')\n",
    "# 转为 numpy array格式，方便后续数据处理\n",
    "data = np.array(data)\n",
    "# 转为 tensor 格式，以允许 pytorch 使用\n",
    "data = torch.tensor(data)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "data_train = data[0:7,:]\n",
    "data_train_X = data_train[:,0:8]\n",
    "data_train_Y = data_train[:,8]\n",
    "\n",
    "data_test = data[7:,:]\n",
    "data_test_X = data_test[:,0:8]\n",
    "data_test_Y = data_test[:,8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[3.0400e-01, 2.6130e+02, 1.1727e+03, 4.9054e+01, 3.6905e+02, 4.5488e+01,\n",
      "         5.0000e+00, 1.0000e+00]], dtype=torch.float64), tensor([537.6667], dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "# 使用TensorDataset函数构建pytorch数据集\n",
    "dataset_train = torch.utils.data.TensorDataset(data_train_X,data_train_Y)\n",
    "dataset_test = torch.utils.data.TensorDataset(data_test_X,data_test_Y)\n",
    "train_set =  torch.utils.data.DataLoader(dataset_train,batch_size=1,shuffle=True)\n",
    "test_set =  torch.utils.data.DataLoader(dataset_test,batch_size=1,shuffle=True)\n",
    "# 测试一下数据集，这里让train_set输出一个样本（包含样本X和标签Y），看看构建的是否正确\n",
    "sample = next(iter(train_set))\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cwdbo\\anaconda3\\envs\\ml_pt2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 MSE_tr: 5481.33984375\n",
      "epoch 100 MSE_tr: 17.577096939086914\n",
      "epoch 150 MSE_tr: 253.224365234375\n",
      "epoch 200 MSE_tr: 1220.4903564453125\n",
      "epoch 250 MSE_tr: 79.58460998535156\n",
      "epoch 300 MSE_tr: 11678.078125\n",
      "epoch 350 MSE_tr: 103.74163818359375\n",
      "epoch 400 MSE_tr: 46993.08984375\n",
      "epoch 450 MSE_tr: 159.3798370361328\n",
      "epoch 500 MSE_tr: 13159.130859375\n",
      "epoch 550 MSE_tr: 54891.96484375\n",
      "epoch 600 MSE_tr: 5614.33935546875\n",
      "epoch 650 MSE_tr: 13836.8828125\n",
      "epoch 700 MSE_tr: 20.959228515625\n",
      "epoch 750 MSE_tr: 29199.015625\n",
      "epoch 800 MSE_tr: 2246.363037109375\n",
      "epoch 850 MSE_tr: 30702.765625\n",
      "epoch 900 MSE_tr: 373.77935791015625\n",
      "epoch 950 MSE_tr: 18.310813903808594\n",
      "epoch 1000 MSE_tr: 4395.38916015625\n",
      "epoch 1050 MSE_tr: 309.7000732421875\n",
      "epoch 1100 MSE_tr: 3406.2939453125\n",
      "epoch 1150 MSE_tr: 2637.525146484375\n",
      "epoch 1200 MSE_tr: 16233.2470703125\n",
      "epoch 1250 MSE_tr: 6789.5830078125\n",
      "epoch 1300 MSE_tr: 29392.76171875\n",
      "epoch 1350 MSE_tr: 43735.93359375\n",
      "epoch 1400 MSE_tr: 12266.0361328125\n",
      "epoch 1450 MSE_tr: 7232.79443359375\n",
      "epoch 1500 MSE_tr: 3356.265869140625\n",
      "epoch 1550 MSE_tr: 10.709799766540527\n",
      "epoch 1600 MSE_tr: 245.5053253173828\n",
      "epoch 1650 MSE_tr: 2387.63818359375\n",
      "epoch 1700 MSE_tr: 102.17848205566406\n",
      "epoch 1750 MSE_tr: 21803.638671875\n",
      "epoch 1800 MSE_tr: 3361.115478515625\n",
      "epoch 1850 MSE_tr: 167.94635009765625\n",
      "epoch 1900 MSE_tr: 1882.44970703125\n",
      "epoch 1950 MSE_tr: 426.89501953125\n",
      "epoch 2000 MSE_tr: 258.406005859375\n"
     ]
    }
   ],
   "source": [
    "# CNN 网络加载\n",
    "net = Network()\n",
    "\n",
    "# 损失函数设置为MSE，也就是均方根误差\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "# 加载数据，设置优化器\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.01)\n",
    "# lr_schedule = torch.optim.lr_scheduler.StepLR(\\\n",
    "#         optimizer, 1, gamma=0.8, last_epoch=-1)\n",
    "\n",
    "# 训练过程\n",
    "# 设置总训练轮次\n",
    "epoch_num = 2000\n",
    "# 把网络送到device(如果有GPU)\n",
    "net.to(device)\n",
    "# 把网络设置为训练模式，这是因为一些特殊层（例如dropout）在训练和测试使用的时候需要不同的特性\n",
    "net.train()\n",
    "# 开始训练\n",
    "for epoch in range(epoch_num):\n",
    "\n",
    "    # 从训练集读取一个batch，batch大小由自己设置\n",
    "    for batch in train_set:        \n",
    "        datas, labels = batch\n",
    "        # 需要转为float tensor才能训练\n",
    "        datas = datas.to(torch.float32)\n",
    "        labels = labels.float()\n",
    "        # 样本datas输入net,成为preds。加上.to(device)是为了确保它在GPU运行\n",
    "        preds = net(datas.to(device)) \n",
    "        # 训练损失的计算由刚刚定义的损失函数负责，需要网络预测的结果preds，和真实数据labels，度量它们的距离\n",
    "        trainloss = criterion(preds.to(device), labels.to(device))\n",
    "        # 固定步骤，更新网络参数前先清零，避免叠加；然后反向传播，再更新参数\n",
    "        optimizer.zero_grad()\n",
    "        trainloss.backward() # Calculate Gradients\n",
    "        optimizer.step() # Update Weight\n",
    "        # lr_schedule.step() # 学习率变化\n",
    "\n",
    "    # 每50个epoch打印一次当前的损失，方便观察\n",
    "    if (epoch+1) % 50 ==0:\n",
    "        print(\n",
    "            \"epoch\", epoch+1, \n",
    "            \"MSE_tr:\", float(trainloss),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1133.1000], dtype=torch.float64) tensor([[2.3096]], grad_fn=<AddmmBackward0>)\n",
      "tensor([322.3667], dtype=torch.float64) tensor([[2.3096]], grad_fn=<AddmmBackward0>)\n",
      "tensor([368.1000], dtype=torch.float64) tensor([[93.5689]], grad_fn=<AddmmBackward0>)\n",
      "tensor([560.4000], dtype=torch.float64) tensor([[145.3111]], grad_fn=<AddmmBackward0>)\n",
      "tensor([361.3000], dtype=torch.float64) tensor([[258.6078]], grad_fn=<AddmmBackward0>)\n",
      "tensor([277.], dtype=torch.float64) tensor([[455.0771]], grad_fn=<AddmmBackward0>)\n",
      "tensor([634.8000], dtype=torch.float64) tensor([[2.3096]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 验证效果\n",
    "# 加载测试集样本，然后与预测的结果比较。\n",
    "for batch in test_set:\n",
    "\n",
    "    test_data_sample_X,test_data_sample_Y = batch\n",
    "    # 把测试X输入网络\n",
    "    net.eval()\n",
    "    net.to('cpu')\n",
    "    # test_data_sample_X = test_data_sample_X.float()\n",
    "    predict = net(test_data_sample_X.float())\n",
    "\n",
    "    print(test_data_sample_Y,predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchlearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "696bc8a05a1eb808e62d18e9328544428937c8e11a4c703581cc2236b7d3d24e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
